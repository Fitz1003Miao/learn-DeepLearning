## AlexNet

### 亮点

1. 引入了 ReLU 线性整流函数作为神经元的激活函数，ReLU 比 Sigmoid 激活函数以及 tanh 激活函数好的地方在于：

   - 可以更快速的收敛。因为 Sigmoid 和 tanh 函数会出现梯度消失的问题，即当激活函数接近饱和区的时候，导数会接近于0。
   - 计算复杂度低，不需要进行幂运算。

2. Local Response Normalization.来源于神经学的“侧抑制”概念，即一个活跃的神经元会抑制它附近的神经元。这个 Normalize 层在与 ReLU 激活函数结合使用时非常有用。通过对局部神经元的活动创建竞争机制，使得其中响应较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。

   $$b^i_{x. y} = a^i_{x, y} / (k + \alpha \sum_{j = max(0, i - n / 2)}^{i = min(N - 1, i + n / 2)}(a^j_{x, y})^2)^\beta$$

   这里 $a^i_{x, y}$表示的是 ReLU激活函数的输出结果，$N$ 表示神经元的个数，$n$ 表示邻近位置上kernel map 的数量。

3. 池化层，用于卷积操作之后，作用是特征融合和降维。最大池化的作用就是提取 Local 区域内的最大特征作为Local 区域的特征，改变图像的宽高，不改变通道数。其中重叠池化会提升正确率。

4. Dropout，为了避免过拟合将每个神经元的输出结果以一定的可能性变为0，模拟当前神经元不工作的情况，增强模型的泛化能力。

### 结构

五层卷积层、三层全连接层

* 第一层卷积核 96 个，大小为11 * 11，stride 为 4 * 4，输出结果为 55 * 55 * 96(按照论文中的说明输入为224 * 224 * 3)，卷积完成之后要进行 ReLU、LRN 均不会改变维度， 再对结果进行最大池化，池化层的kernel size 为 3 * 3, stride 为 2 * 2，输出结果为 27 * 27 * 96
* 第二层卷积核 128个，大小为 5 * 5， stride 为 1 * 1，padding 为2，输出结果为2 * 27 * 27 *128，进行ReLU、LRN，最大池化，最终结果为2 * 13 * 13 * 128
* 第三层 卷积核384个，大小为 3 * 3，stride 为 1* 1，padding为1，输出结果为13 * 13 * 384，进行Re'LU
* 第四层 卷积核 192 个，大小为 3 * 3， stride 为1 * 1，padding 为1， 输出结果为 13 * 13 * 192，进行ReLU
* 第五层 卷积核 256 个，大小为 3 * 3， stride 为1 * 1， padding 为1，输出结果为 13 * 13 * 256，进行ReLU之后，再进行最大池化，最终结果为6 * 6 * 256
* 第一层全连接层 256 * 6 *6 -- 4096，Dropout
* 第二层全连接层 4096 -- 4096，Dropout
* 第三层全连接 4096 -- CLASS_NUM